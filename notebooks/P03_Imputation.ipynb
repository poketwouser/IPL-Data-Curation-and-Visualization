{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Imputation and Data Consolidation\n",
                "\n",
                "This notebook merges the processed match data with scrapped time data, imputes missing values, and consolidates the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from datetime import datetime, timedelta\n",
                "import os\n",
                "\n",
                "# Ensure the processed data directory exists\n",
                "os.makedirs('../data/processed', exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def trim_and_title(df):\n",
                "    \"\"\"Standardize column names and string values.\"\"\"\n",
                "    df.columns = df.columns.str.strip().str.title()\n",
                "    for col in df.columns:\n",
                "        if df[col].dtype == 'object':\n",
                "            df[col] = df[col].str.strip().str.title()\n",
                "    return df\n",
                "\n",
                "def standardize_teams(df, cols):\n",
                "    \"\"\"Standardize IPL team names.\"\"\"\n",
                "    team_name_map = {\n",
                "        'Deccan Chargers': 'Sunrisers Hyderabad',\n",
                "        'Delhi Daredevils': 'Delhi Capitals',\n",
                "        'Royal Challengers Bengaluru': 'Royal Challengers Bangalore',\n",
                "        'Kings XI Punjab': 'Punjab Kings',\n",
                "        'Kings Xi Punjab': 'Punjab Kings',\n",
                "        'Rising Pune Supergiants': 'Rising Pune Supergiant',\n",
                "        'Pune Warriors': 'Pune Warriors India'\n",
                "    }\n",
                "    for col in cols:\n",
                "        if col in df.columns:\n",
                "             df[col] = df[col].replace(team_name_map)\n",
                "    return df\n",
                "\n",
                "def clean_time(time_series):\n",
                "    \"\"\"Clean time strings efficiently.\"\"\"\n",
                "    if time_series.dtype != 'object':\n",
                "        return time_series\n",
                "        \n",
                "    # Vectorized cleaning\n",
                "    cleaned = time_series.astype(str).str.strip().str.lower()\n",
                "    cleaned = cleaned.str.replace('.', '', regex=False)\n",
                "    cleaned = cleaned.str.replace(' ', '', regex=False)\n",
                "    cleaned = cleaned.str.replace('am', ' am', regex=False)\n",
                "    cleaned = cleaned.str.replace('pm', ' pm', regex=False)\n",
                "    \n",
                "    # Remove leading zeros (e.g., '08:00' -> '8:00')\n",
                "    return cleaned.apply(lambda x: x.lstrip('0') if pd.notna(x) else x)\n",
                "\n",
                "def ist_to_local(df, time_col, city_col):\n",
                "    \"\"\"Convert IST time to local time based on City.\"\"\"\n",
                "    city_to_offset = {\n",
                "        # South Africa (2009)\n",
                "        'Cape Town': -3.5, 'Port Elizabeth': -3.5, 'Durban': -3.5,\n",
                "        'Centurion': -3.5, 'East London': -3.5, 'Johannesburg': -3.5,\n",
                "        'Kimberley': -3.5, 'Bloemfontein': -3.5,\n",
                "        # UAE (2014, 2020, 2021)\n",
                "        'Abu Dhabi': -1.5, 'Dubai': -1.5, 'Sharjah': -1.5\n",
                "    }\n",
                "    # Default offset is 0 for India\n",
                "    \n",
                "    def convert(row):\n",
                "        time_str = row[time_col]\n",
                "        city = row[city_col]\n",
                "        \n",
                "        if pd.isna(time_str):\n",
                "            return None\n",
                "            \n",
                "        offset = city_to_offset.get(city, 0)\n",
                "        if offset == 0:\n",
                "            return time_str\n",
                "            \n",
                "        try:\n",
                "            t = datetime.strptime(str(time_str), \"%I:%M %p\")\n",
                "            local_t = t + timedelta(hours=offset)\n",
                "            return local_t.strftime(\"%I:%M %p\").lstrip(\"0\").lower()\n",
                "        except:\n",
                "            return time_str\n",
                "\n",
                "    return df.apply(convert, axis=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Matches loaded from parquet.\n",
                        "Scrapped matches loaded.\n"
                    ]
                }
            ],
            "source": [
                "# Load processed parquet files\n",
                "try:\n",
                "    matches = pd.read_parquet('../data/processed/matches.parquet')\n",
                "    print(\"Matches loaded from parquet.\")\n",
                "    # Standardize teams again to ensure consistency (e.g. handle 'Kings Xi Punjab')\n",
                "    matches = standardize_teams(matches, ['Team1', 'Team2'])\n",
                "except FileNotFoundError:\n",
                "    print(\"Matches parquet not found. Please run P01_Pre_Processing.ipynb first.\")\n",
                "\n",
                "# Load scrapped data\n",
                "try:\n",
                "    all_matches = pd.read_csv('../data/scrapped/all_matches.csv')\n",
                "    print(\"Scrapped matches loaded.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Scrapped data not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clean Scrapped Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean all_matches\n",
                "all_matches = trim_and_title(all_matches)\n",
                "all_matches = standardize_teams(all_matches, ['Team 1', 'Team 2'])\n",
                "all_matches['Time'] = clean_time(all_matches['Time'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fix Known Data Issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Issue 1: Missing Match 24 in 2012 in scrapped data\n",
                "# Adding it manually to all_matches before merging\n",
                "new_row = {\n",
                "    'Match': 'Match 24',\n",
                "    'Team 1': 'Chennai Super Kings',\n",
                "    'Team 2': 'Pune Warriors India',\n",
                "    'Date': pd.to_datetime('2012-04-19'),\n",
                "    'Time': '8:00 pm',\n",
                "    'Season': 2012\n",
                "}\n",
                "\n",
                "all_matches['Date'] = pd.to_datetime(all_matches['Date'], errors='coerce')\n",
                "\n",
                "# Append new row\n",
                "all_matches = pd.concat([all_matches, pd.DataFrame([new_row])], ignore_index=True)\n",
                "\n",
                "# Issue 2: Incorrect date in matches dataset for Match 734043 (Reserve day used)\n",
                "matches.loc[matches['Id'] == 734043, 'Date'] = pd.to_datetime('2014-05-28')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare for Merge"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure Date Format is consistent (Date object)\n",
                "matches['Date'] = pd.to_datetime(matches['Date']).dt.date\n",
                "all_matches['Date'] = pd.to_datetime(all_matches['Date']).dt.date\n",
                "\n",
                "# Create Match Key: Tuple of sorted team names to handle team order differences\n",
                "matches['match_key'] = matches.apply(lambda x: tuple(sorted([x['Team1'], x['Team2']])), axis=1)\n",
                "all_matches['match_key'] = all_matches.apply(lambda x: tuple(sorted([x['Team 1'], x['Team 2']])), axis=1)\n",
                "\n",
                "# Rename columns in all_matches for clarity\n",
                "all_matches = all_matches.rename(columns={'Match': 'Match_No'})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Merge Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Correctly Merged: 1095\n",
                        "Abandoned/Unmatched (from scrapped): 11\n"
                    ]
                }
            ],
            "source": [
                "# Merge matches with scrapped time and match number\n",
                "merged = pd.merge(\n",
                "    matches,\n",
                "    all_matches[['Season', 'Date', 'match_key', 'Time', 'Match_No']],\n",
                "    on=['Season', 'Date', 'match_key'],\n",
                "    how='outer', # Using outer to catch the abandoned matches that might be in scrapped but not in matches\n",
                "    indicator=True\n",
                ")\n",
                "\n",
                "# Split into matched and unmatched\n",
                "matches_data = merged[merged['_merge'] == 'both'].copy()\n",
                "matches_data.drop(columns=['_merge'], inplace=True)\n",
                "\n",
                "# Rows in scrapped (all_matches) but not in matches (processed) -> Likely abandoned matches without results\n",
                "abandoned_matches = merged[merged['_merge'] == 'right_only'].copy()\n",
                "abandoned_matches.drop(columns=['_merge'], inplace=True)\n",
                "\n",
                "print(f\"Correctly Merged: {len(matches_data)}\")\n",
                "print(f\"Abandoned/Unmatched (from scrapped): {len(abandoned_matches)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Incorporate Abandoned Matches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fill missing columns for abandoned matches\n",
                "if not abandoned_matches.empty:\n",
                "    # Extract Teams from match_key\n",
                "    abandoned_matches['Team1'] = abandoned_matches['match_key'].apply(lambda x: x[0])\n",
                "    abandoned_matches['Team2'] = abandoned_matches['match_key'].apply(lambda x: x[1])\n",
                "    \n",
                "    # Set default values for abandoned matches\n",
                "    defaults = {\n",
                "        'Match_Type': 'League', # Assumption, or could map if known\n",
                "        'Toss_Winner': 'No Toss',\n",
                "        'Toss_Decision': 'No Toss',\n",
                "        'Winner': 'No Result',\n",
                "        'Result': 'No Result',\n",
                "        'Player_Of_Match': 'No Result',\n",
                "        'Venue': 'Unknown', # Could imply from city if we had a map, or leave unknown\n",
                "        'Result_Margin': -1,\n",
                "        'Target_Runs': -1,\n",
                "        'Target_Overs': -1,\n",
                "        'Super_Over': 'N',\n",
                "        'Method': 'No Result'\n",
                "    }\n",
                "    abandoned_matches = abandoned_matches.fillna(defaults)\n",
                "    \n",
                "    # Generate dummy IDs for these matches to avoid nulls (start from max id + 1)\n",
                "    max_id = matches['Id'].max()\n",
                "    abandoned_matches['Id'] = range(max_id + 1, max_id + 1 + len(abandoned_matches))\n",
                "\n",
                "    # Concatenate back\n",
                "    final_df = pd.concat([matches_data, abandoned_matches], ignore_index=True)\n",
                "else:\n",
                "    final_df = matches_data.copy()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Impute Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Missing Times: 0\n"
                    ]
                }
            ],
            "source": [
                "# Impute Time (IST to Local)\n",
                "final_df['Time'] = ist_to_local(final_df, 'Time', 'City')\n",
                "\n",
                "# Validate imputation\n",
                "print(\"Missing Times:\", final_df['Time'].isna().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Consolidated Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Success: Consolidated data saved to ../data/processed/matches_imputed.parquet\n"
                    ]
                }
            ],
            "source": [
                "# Select and Reorder columns\n",
                "cols = [\n",
                "    'Id', 'Season', 'Match_No', 'Date', 'Time', 'City', 'Venue', \n",
                "    'Team1', 'Team2', 'Toss_Winner', 'Toss_Decision', \n",
                "    'Winner', 'Result', 'Result_Margin', 'Target_Runs', 'Target_Overs', \n",
                "    'Player_Of_Match', 'Match_Type', 'Super_Over', 'Method', \n",
                "    'Umpire1', 'Umpire2'\n",
                "]\n",
                "\n",
                "# Ensure all columns exist\n",
                "for col in cols:\n",
                "    if col not in final_df.columns:\n",
                "        final_df[col] = None\n",
                "\n",
                "final_df = final_df[cols].sort_values(['Season', 'Date']).reset_index(drop=True)\n",
                "\n",
                "# Save\n",
                "try:\n",
                "    final_df.to_parquet('../data/processed/matches_imputed.parquet', index=False)\n",
                "    print(\"Success: Consolidated data saved to ../data/processed/matches_imputed.parquet\")\n",
                "except Exception as e:\n",
                "    print(f\"Error saving parquet: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 1106 entries, 0 to 1105\n",
                        "Data columns (total 22 columns):\n",
                        " #   Column           Non-Null Count  Dtype  \n",
                        "---  ------           --------------  -----  \n",
                        " 0   Id               1106 non-null   float64\n",
                        " 1   Season           1106 non-null   int64  \n",
                        " 2   Match_No         1106 non-null   object \n",
                        " 3   Date             1106 non-null   object \n",
                        " 4   Time             1106 non-null   object \n",
                        " 5   City             1095 non-null   object \n",
                        " 6   Venue            1106 non-null   object \n",
                        " 7   Team1            1106 non-null   object \n",
                        " 8   Team2            1106 non-null   object \n",
                        " 9   Toss_Winner      1106 non-null   object \n",
                        " 10  Toss_Decision    1106 non-null   object \n",
                        " 11  Winner           1106 non-null   object \n",
                        " 12  Result           1106 non-null   object \n",
                        " 13  Result_Margin    1106 non-null   float64\n",
                        " 14  Target_Runs      1106 non-null   float64\n",
                        " 15  Target_Overs     1106 non-null   float64\n",
                        " 16  Player_Of_Match  1106 non-null   object \n",
                        " 17  Match_Type       1106 non-null   object \n",
                        " 18  Super_Over       1106 non-null   object \n",
                        " 19  Method           1106 non-null   object \n",
                        " 20  Umpire1          1095 non-null   object \n",
                        " 21  Umpire2          1095 non-null   object \n",
                        "dtypes: float64(4), int64(1), object(17)\n",
                        "memory usage: 190.2+ KB\n"
                    ]
                }
            ],
            "source": [
                "final_df.info()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
